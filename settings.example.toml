# VoiceType Configuration File - Example
# Copy this file to settings.toml and customize as needed

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Optional: Specify a custom path for the log file.
# If not specified, uses platform defaults:
#   - Linux: ~/.config/voicetype/voicetype.log
#   - macOS: ~/Library/Application Support/voicetype/voicetype.log
#   - Windows: %APPDATA%/voicetype/voicetype.log

# log_file = "/path/to/custom/voicetype.log"
# log_file = "~/custom/logs/voicetype.log"  # Tilde expansion supported

# =============================================================================
# STAGE DEFINITIONS
# =============================================================================
# Define named stage instances with their configurations.
# Stages can be referenced by name in pipelines and reused across pipelines.
#
# IMPORTANT: 'stage_class' is a RESERVED field name used to specify which
# stage class to instantiate. Stage implementations cannot use 'stage_class'
# as a configuration parameter name.

[stage_configs.RecordAudio_default]
stage_class = "RecordAudio"
minimum_duration = 0.25  # Minimum audio duration in seconds

[stage_configs.Transcribe_local]
stage_class = "Transcribe"
provider = "local"  # Use local faster-whisper model

[stage_configs.Transcribe_cloud]
stage_class = "Transcribe"
provider = "litellm"  # Use cloud-based transcription (requires API key)

[stage_configs.CorrectTypos_default]
stage_class = "CorrectTypos"
case_sensitive = false
whole_word_only = true
corrections = [
    ["machinelearning", "machine learning"],
    ["air quotes", "error codes"],
    ["Python", "python", "case_sensitive=true"],
]

[stage_configs.TypeText_default]
stage_class = "TypeText"

[stage_configs.LLMAgent_jarvis]
stage_class = "LLMAgent"
provider = "openai:gpt-4o-mini"  # or "ollama:llama3.2", "anthropic:claude-sonnet-4-5", etc.
system_prompt = """You are a text transformation assistant.
When you see a trigger word like 'jarvis' or 'hey assistant' in the input:
- Text BEFORE the trigger word is the content to transform
- Text AFTER the trigger word is the instruction for how to transform it
- Apply the instruction and return ONLY the transformed text

If there's no trigger word in the input, return the original text unchanged.

Examples:
- Input: "This is my email jarvis make it professional"
  Output: "Dear [Recipient],\n\nThis is my email.\n\nBest regards"
- Input: "Just regular typing"
  Output: "Just regular typing"
"""
temperature = 0.3
# max_tokens = 500  # Optional: limit response length
# timeout = 30  # Optional: request timeout in seconds
# fallback_on_error = true  # Optional: return original text on error (default: true)

# =============================================================================
# PIPELINE CONFIGURATION
# =============================================================================
# Pipelines define sequences of stages that process voice input.
# Multiple pipelines can be configured with different hotkeys.
#
# Pipelines reference stage instances by name (easier to override/reuse).

# Example 1: Default voice-to-text pipeline
[[pipelines]]
name = "default"
enabled = true
hotkey = "<pause>"  # Pause/Break key
stages = [
    "RecordAudio_default",
    "Transcribe_local",
    "CorrectTypos_default",
    "TypeText_default",
]

# Example 2: Alternative pipeline with cloud transcription (commented out)
# [[pipelines]]
# name = "cloud_transcribe"
# enabled = false
# hotkey = "<f12>"
# stages = [
#     "RecordAudio_default",
#     "Transcribe_cloud",  # Different transcription provider
#     "TypeText_default",
# ]

# Example 3: Voice assistant mode with LLM processing
# [[pipelines]]
# name = "jarvis_mode"
# enabled = false
# hotkey = "<ctrl>+<pause>"
# stages = [
#     "RecordAudio_default",
#     "Transcribe_local",
#     "LLMAgent_jarvis",  # Process with LLM agent
#     "TypeText_default",
# ]

# Example 4: Pipeline with multiple instances of same stage
# To use a stage twice with different configs, create separate named instances
# [stage_configs.CorrectTypos_slang]
# stage_class = "CorrectTypos"
# corrections = [["gonna", "going to"], ["wanna", "want to"]]
#
# [stage_configs.CorrectTypos_technical]
# stage_class = "CorrectTypos"
# corrections = [["machinelearning", "machine learning"]]
#
# [[pipelines]]
# name = "multi_correct"
# enabled = false
# hotkey = "<ctrl>+<pause>"
# stages = [
#     "RecordAudio_default",
#     "Transcribe_local",
#     "CorrectTypos_slang",      # First correction pass
#     "CorrectTypos_technical",  # Second correction pass
#     "TypeText_default",
# ]


# =============================================================================
# CONFIGURATION NOTES
# =============================================================================
#
# Available Stages:
#   - RecordAudio: Records audio while hotkey is held down
#   - Transcribe: Converts audio to text using speech recognition
#   - CorrectTypos: Corrects configured typos and common speech-to-text errors
#   - LLMAgent: Processes text through an LLM agent (local or remote)
#   - TypeText: Types the transcribed text at the current cursor position
#
# Hotkey Format:
#   - Special keys: <pause>, <f1>, <f12>, etc.
#   - Modifiers: <ctrl>+<alt>+p, <shift>+<f1>, etc.
#   - Regular keys: a, b, 1, 2, etc.
#
# Transcription Provider Options:
#   - "local": Uses faster-whisper for local transcription (faster, offline)
#   - "litellm": Uses cloud-based transcription (requires API key)
#
# LLM Provider Options (for LLMAgent stage):
#   - Local: "ollama:llama3.2", "ollama:mistral", "ollama:phi3" (requires Ollama installed)
#   - Remote: "openai:gpt-4o", "anthropic:claude-sonnet-4-5", "gemini:gemini-2.0-flash", etc.
#   - See Pydantic AI docs for full list: https://ai.pydantic.dev/models/
#
# File Locations (searched in order):
#   1. ./settings.toml (current directory)
#   2. ~/.config/voicetype/settings.toml (user config)
#   3. /etc/voicetype/settings.toml (system-wide)
